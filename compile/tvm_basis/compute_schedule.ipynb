{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceae7fa9",
   "metadata": {},
   "source": [
    "安装tvm, 参考官方文档: https://tvm.apache.org/docs/install/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40c2b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-tvm in /Applications/anaconda3/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: scipy in /Applications/anaconda3/lib/python3.9/site-packages (from apache-tvm) (1.7.3)\n",
      "Requirement already satisfied: synr==0.6.0 in /Applications/anaconda3/lib/python3.9/site-packages (from apache-tvm) (0.6.0)\n",
      "Requirement already satisfied: psutil in /Applications/anaconda3/lib/python3.9/site-packages (from apache-tvm) (5.8.0)\n",
      "Requirement already satisfied: tornado in /Applications/anaconda3/lib/python3.9/site-packages (from apache-tvm) (6.1)\n",
      "Requirement already satisfied: cloudpickle in /Applications/anaconda3/lib/python3.9/site-packages (from apache-tvm) (2.0.0)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/lib/python3.9/site-packages (from apache-tvm) (1.21.5)\n",
      "Requirement already satisfied: decorator in /Applications/anaconda3/lib/python3.9/site-packages (from apache-tvm) (5.1.1)\n",
      "Requirement already satisfied: attrs in /Applications/anaconda3/lib/python3.9/site-packages (from apache-tvm) (21.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install apache-tvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3affa29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "\n",
    "import tvm\n",
    "from tvm import te\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26cb5b",
   "metadata": {},
   "source": [
    "和tensorflow的静态图的概念类似，TVM通过compute描述tensor的操作，用schedule描述在硬件上的调度，最后通过build完成编译。简单的理解是compute影响计算的结果，schedule影响计算的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc529e",
   "metadata": {},
   "source": [
    "# compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae65e15",
   "metadata": {},
   "source": [
    "使用compute描述一个tensor的加法和减法，并用tvm.lower打印出计算描述信息。\n",
    "* 在定义TVM placeholders的时候指定数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c568ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float16), float16, [12], []),\n",
      "             B: Buffer(B_2: Pointer(float16), float16, [12], []),\n",
      "             C: Buffer(C_2: Pointer(float16), float16, [12], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float16, [3, 4], []), B_1: B_3: Buffer(B_2, float16, [3, 4], []), C_1: C_3: Buffer(C_2, float16, [3, 4], [])} {\n",
      "  for (i0: int32, 0, 3) {\n",
      "    for (i1: int32, 0, 4) {\n",
      "      let cse_var_1: int32 = ((i0*4) + i1)\n",
      "      C[cse_var_1] = (A[cse_var_1] + B[cse_var_1])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "a [[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]] \n",
      "b [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]] \n",
      "c [[ 1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "def add(shape, dtype):\n",
    "    A = te.placeholder(shape, dtype=dtype, name=\"A\")\n",
    "    B = te.placeholder(shape, dtype=dtype, name=\"B\")\n",
    "    C = te.compute(shape, lambda *index: A[index] + B[index], name=\"C\")\n",
    "    s = te.create_schedule(C.op)\n",
    "    print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "    return tvm.build(s, [A, B, C])\n",
    "    \n",
    "op = add((3, 4), \"float16\")\n",
    "a = tvm.nd.array(np.arange(12, dtype='float16').reshape((3, 4)))\n",
    "b = tvm.nd.array(np.ones((3, 4)).astype('float16'))\n",
    "c = tvm.nd.array(np.empty((3, 4), dtype='float16'))\n",
    "op(a, b, c)\n",
    "print(\"a\", a, '\\nb', b, '\\nc', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c282b9",
   "metadata": {},
   "source": [
    "* 在compute过程中用astype转换数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec44c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [12], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [12], []),\n",
      "             C: Buffer(C_2: Pointer(float16), float16, [12], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [3, 4], []), B_1: B_3: Buffer(B_2, float32, [3, 4], []), C_1: C_3: Buffer(C_2, float16, [3, 4], [])} {\n",
      "  for (i0: int32, 0, 3) {\n",
      "    for (i1: int32, 0, 4) {\n",
      "      let cse_var_1: int32 = ((i0*4) + i1)\n",
      "      C[cse_var_1] = (cast(float16, A[cse_var_1]) - cast(float16, B[cse_var_1]))\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "a [[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]] \n",
      "b [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]] \n",
      "c [[-1.  0.  1.  2.]\n",
      " [ 3.  4.  5.  6.]\n",
      " [ 7.  8.  9. 10.]]\n"
     ]
    }
   ],
   "source": [
    "def sub(shape, dtype):\n",
    "    A = te.placeholder(shape, name=\"A\")\n",
    "    B = te.placeholder(shape, name=\"B\")\n",
    "    C = te.compute(shape, lambda *index: A[index].astype(dtype) - B[index].astype(dtype), name=\"C\")\n",
    "    s = te.create_schedule(C.op)\n",
    "    print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "    return tvm.build(s, [A, B, C])\n",
    "    \n",
    "op = sub((3, 4), \"float16\")\n",
    "a = tvm.nd.array(np.arange(12, dtype='float32').reshape((3, 4)))\n",
    "b = tvm.nd.array(np.ones((3, 4)).astype('float32'))\n",
    "c = tvm.nd.array(np.empty((3, 4), dtype='float16'))\n",
    "op(a, b, c)\n",
    "print(\"a\", a, '\\nb', b, '\\nc', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42754fd",
   "metadata": {},
   "source": [
    "* shape未知时用te.var()来表示tensor的维度大小."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d152ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_3: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_4: int32], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m, n], [stride_2, stride_5: int32], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    for (j: int32, 0, n) {\n",
      "      C[((i*stride_2) + (j*stride_5))] = (A[((i*stride) + (j*stride_3))] + B[((i*stride_1) + (j*stride_4))])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dynamic_add():\n",
    "    # declare some variables for use later\n",
    "    n = te.var(\"n\")\n",
    "    m = te.var(\"m\")\n",
    "    # declare a matrix element-wise multiply\n",
    "    A = te.placeholder((m, n), name=\"A\")\n",
    "    B = te.placeholder((m, n), name=\"B\")\n",
    "    C = te.compute((m, n), lambda i, j: A[i, j] + B[i, j], name=\"C\")\n",
    "    s = te.create_schedule([C.op])\n",
    "    print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "    return tvm.build(s, [A, B, C])\n",
    "op = dynamic_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c25ff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]] \n",
      "b [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]] \n",
      "c [[ 1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "a = tvm.nd.array(np.arange(12, dtype='float32').reshape((3, 4)))\n",
    "b = tvm.nd.array(np.ones((3, 4)).astype('float32'))\n",
    "c = tvm.nd.array(np.empty((3, 4), dtype='float32'))\n",
    "op(a, b, c)\n",
    "print(\"a\", a, '\\nb', b, '\\nc', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b8b40c",
   "metadata": {},
   "source": [
    "* 变量的compute支持运行时传递任意shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19a7cd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[0. 1. 2. 3. 4.]\n",
      " [5. 6. 7. 8. 9.]] \n",
      "b [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]] \n",
      "c [[ 1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10.]]\n"
     ]
    }
   ],
   "source": [
    "a = tvm.nd.array(np.arange(10, dtype='float32').reshape((2, 5)))\n",
    "b = tvm.nd.array(np.ones((2, 5)).astype('float32'))\n",
    "c = tvm.nd.array(np.empty((2, 5), dtype='float32'))\n",
    "op(a, b, c)\n",
    "print(\"a\", a, '\\nb', b, '\\nc', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77df48d8",
   "metadata": {},
   "source": [
    "* 交换tensor的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8df477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*n: int32)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [n, m], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i: int32, 0, n) {\n",
      "    for (j: int32, 0, m) {\n",
      "      B[((i*stride_1) + (j*stride_3))] = A[((j*stride) + (i*stride_2))]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "a [[0. 1. 2. 3. 4.]\n",
      " [5. 6. 7. 8. 9.]] \n",
      "b [[0. 5.]\n",
      " [1. 6.]\n",
      " [2. 7.]\n",
      " [3. 8.]\n",
      " [4. 9.]]\n"
     ]
    }
   ],
   "source": [
    "def transpose():\n",
    "    n = te.var(\"n\")\n",
    "    m = te.var(\"m\")\n",
    "    A = te.placeholder((m, n), name=\"A\")\n",
    "    B = te.compute((n, m), lambda i, j: A[j, i], name=\"B\")\n",
    "    s = te.create_schedule([B.op])\n",
    "    print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "    return tvm.build(s, [A, B])\n",
    "op = transpose()\n",
    "a = tvm.nd.array(np.arange(10, dtype='float32').reshape((2, 5)))\n",
    "b = tvm.nd.array(np.empty((5, 2)).astype('float32'))\n",
    "op(a, b)\n",
    "print(\"a\", a, '\\nb', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa105b35",
   "metadata": {},
   "source": [
    "* 改变tensor的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "267bd6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*p: int32)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [p, q: int32], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i: int32, 0, p) {\n",
      "    for (j: int32, 0, q) {\n",
      "      B[((i*stride_1) + (j*stride_3))] = A[((floordiv(((i*q) + j), n)*stride) + (floormod(((i*q) + j), n)*stride_2))]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "a [[ 0.  1.  2.]\n",
      " [ 3.  4.  5.]\n",
      " [ 6.  7.  8.]\n",
      " [ 9. 10. 11.]] \n",
      "b [[ 0.  1.  2.]\n",
      " [ 3.  4.  5.]\n",
      " [ 6.  7.  8.]\n",
      " [ 9. 10. 11.]]\n"
     ]
    }
   ],
   "source": [
    "def reshape():\n",
    "    n, m = te.var(\"n\"), te.var(\"m\")\n",
    "    p, q = te.var('p'), te.var('q')\n",
    "    A = te.placeholder((m, n), name=\"A\")\n",
    "    B = te.compute((p, q), lambda i, j: A[(i*q+j)//n, (i*q+j)%n], name=\"B\")\n",
    "    s = te.create_schedule([B.op])\n",
    "    print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "    return tvm.build(s, [A, B])\n",
    "op = reshape()\n",
    "a = tvm.nd.array(np.arange((12), dtype='float32').reshape(4, 3))\n",
    "b = tvm.nd.array(np.empty((4, 3)).astype('float32'))\n",
    "op(a, b)\n",
    "print(\"a\", a, '\\nb', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bcae6e",
   "metadata": {},
   "source": [
    "* 维度和下标都可以用变量来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df0515b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(floordiv((n - bi: int32), si: int32)*floordiv((m: int32 - bj: int32), sj: int32))], [])}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n, m], [stride, stride_1: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [floordiv((n - bi), si), floordiv((m - bj), sj)], [])} {\n",
      "  for (i: int32, 0, floordiv((n - bi), si)) {\n",
      "    for (j: int32, 0, floordiv((m - bj), sj)) {\n",
      "      B[((i*floordiv((m - bj), sj)) + j)] = A[((((i*si) + bi)*stride) + (((j*sj) + bj)*stride_1))]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "a [[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]] \n",
      "b [[5. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "def slicing():\n",
    "    n, m, bi, bj, si, sj = [te.var(name) for name in ['n', 'm', 'bi', 'bj', 'si', 'sj']]\n",
    "    A = te.placeholder((n, m), name='A')\n",
    "    B = te.compute(((n - bi) // si, (m - bj) // sj), lambda i, j: A[i * si + bi, j * sj + bj], name='B')\n",
    "    s = te.create_schedule([B.op])\n",
    "    print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "    return tvm.build(s, [A, B, bi, bj, si, sj])\n",
    "op = slicing()\n",
    "a = tvm.nd.array(np.arange((12), dtype='float32').reshape(3, 4))\n",
    "b = tvm.nd.array(np.empty((1, 3)).astype('float32'))\n",
    "op(a, b, 1, 1, 2, 1)\n",
    "print(\"a\", a, '\\nb', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a208b",
   "metadata": {},
   "source": [
    "* reduce_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c56d2ee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() takes at least 1 positional argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tvm\u001b[38;5;241m.\u001b[39mlower(s, [A, B], simple_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tvm\u001b[38;5;241m.\u001b[39mbuild(s, [A, B])\n\u001b[0;32m----> 9\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m a \u001b[38;5;241m=\u001b[39m tvm\u001b[38;5;241m.\u001b[39mnd\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39marange((\u001b[38;5;241m12\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     11\u001b[0m b \u001b[38;5;241m=\u001b[39m tvm\u001b[38;5;241m.\u001b[39mnd\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m3\u001b[39m,))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() takes at least 1 positional argument (0 given)"
     ]
    }
   ],
   "source": [
    "def reduction_sum():\n",
    "    n, m = te.var('n'), te.var('m')\n",
    "    A = te.placeholder((n, m), name='A')\n",
    "    j = te.reduce_axis((0, m), name='j')\n",
    "    B = te.compute((n,), lambda i: te.sum(A[i, j], axis=j), name='b')\n",
    "    s = te.create_schedule(B.op)\n",
    "    print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "    return tvm.build(s, [A, B])\n",
    "op = sum()\n",
    "a = tvm.nd.array(np.arange((12), dtype='float32').reshape(3, 4))\n",
    "b = tvm.nd.array(np.empty((3,)).astype('float32'))\n",
    "op(a, b)\n",
    "print(\"a\", a, '\\nb', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab06636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(dtype):\n",
    "    m, n, l = [te.var(name) for name in ['m', 'n', 'l']]\n",
    "    A = te.placeholder((m, l), name=\"A\", dtype=dtype)\n",
    "    B = te.placeholder((l, n), name=\"B\", dtype=dtype)\n",
    "    k = te.reduce_axis((0, l), name='k')\n",
    "    C = te.compute((m, n), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name=\"C\")\n",
    "    s = te.create_schedule(C.op)\n",
    "    print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "    return tvm.build(s, [A, B, C])\n",
    "op = matmul('int8')\n",
    "a = tvm.nd.array(np.arange((6), dtype='int8').reshape(2, 3))\n",
    "b = tvm.nd.array(np.arange((6), dtype='int8').reshape(3, 2))\n",
    "c = tvm.nd.array(np.empty((2, 2), dtype='int8'))\n",
    "op(a, b, c)\n",
    "print(\"a\", a, '\\nb', b, '\\nc', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da08ba6",
   "metadata": {},
   "source": [
    "* 条件表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dcef009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(placeholder_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {placeholder: Buffer(placeholder_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [(stride_1: int32*n)], [], type=\"auto\")}\n",
      "  buffer_map = {placeholder_1: placeholder, compute_1: compute}\n",
      "  preflattened_buffer_map = {placeholder_1: placeholder_3: Buffer(placeholder_2, float32, [n, m: int32], [stride, stride_2: int32], type=\"auto\"), compute_1: compute_3: Buffer(compute_2, float32, [n, m], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i: int32, 0, n) {\n",
      "    for (j: int32, 0, m) {\n",
      "      compute[((i*stride_1) + (j*stride_3))] = @tir.if_then_else((j <= i), placeholder[((i*stride) + (j*stride_2))], 0f32, dtype=float32)\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "a [[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]] \n",
      "b [[ 0.  0.  0.  0.]\n",
      " [ 4.  5.  0.  0.]\n",
      " [ 8.  9. 10.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def if_express():\n",
    "    n, m = te.var('n'), te.var('m')\n",
    "    A = te.placeholder((n, m))\n",
    "    B = te.compute(A.shape, lambda i, j: te.if_then_else(i >= j, A[i, j], 0.0))\n",
    "    s = te.create_schedule(B.op)\n",
    "    print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "    return tvm.build(s, [A, B])\n",
    "op = if_express()\n",
    "a = tvm.nd.array(np.arange(12).reshape((3, 4)).astype('float32'))\n",
    "b = tvm.nd.array(np.empty((3, 4), dtype='float32'))\n",
    "op(a, b)\n",
    "print(\"a\", a, '\\nb', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966e85c",
   "metadata": {},
   "source": [
    "* all和any表示条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d79e710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [((n + 4)*(m: int32 + 4))], [])}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n, m], [stride, stride_1: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [(n + 4), (m + 4)], [])} {\n",
      "  for (i: int32, 0, (n + 4)) {\n",
      "    for (j: int32, 0, (m + 4)) {\n",
      "      B[((i*(m + 4)) + j)] = @tir.if_then_else(((((i < 2) || ((n + 2) <= i)) || (j < 2)) || ((m + 2) <= j)), 0f32, A[(((i - 2)*stride) + ((j - 2)*stride_1))], dtype=float32)\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "a [[1. 1.]\n",
      " [1. 1.]] \n",
      "b [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def padding(pad):\n",
    "    n, m = te.var('n'), te.var('m')\n",
    "    A = te.placeholder((n, m), name='A')\n",
    "    B = te.compute((n + pad * 2, m + pad * 2),\n",
    "                    lambda i, j: te.if_then_else(\n",
    "                        te.any(i < pad, i >= n + pad, j < pad, j >= m + pad), 0, A[i - pad, j - pad]),\n",
    "                    name='B')\n",
    "    s = te.create_schedule(B.op)\n",
    "    print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "    return tvm.build(s, [A, B])\n",
    "op = padding(2)\n",
    "a = tvm.nd.array(np.ones((2, 2), dtype='float32'))\n",
    "b = tvm.nd.array(np.empty((6, 6), dtype='float32'))\n",
    "op(a, b)\n",
    "print('a', a, '\\nb', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fdc56",
   "metadata": {},
   "source": [
    "# schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b8afd",
   "metadata": {},
   "source": [
    "通常存在多种计算相同结果的方法，但是，不同的方法会导致不同的局部性和性能。因此 TVM 要求用户提供如何执行称为 Schedule 的计算。 时间表是一组计算转换，它转换程序中的计算循环。\n",
    "可以从操作列表创建调度，默认情况下，调度以行优先顺序以串行方式计算张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d07ba9",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c09ef554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m], [stride_1], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((m + 31), 32)) {\n",
      "    for (i.inner: int32, 0, 32) {\n",
      "      if @tir.likely((((i.outer*32) + i.inner) < m), dtype=bool) {\n",
      "        let cse_var_1: int32 = ((i.outer*32) + i.inner)\n",
      "        B[(cse_var_1*stride_1)] = (A[(cse_var_1*stride)]*2f32)\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split can split a specified axis into two axes by factor.\n",
    "m, n = te.var(\"m\"), te.var(\"n\")\n",
    "A = te.placeholder((m,), name=\"A\")\n",
    "B = te.compute((m,), lambda i: A[i] * 2, name=\"B\")\n",
    "\n",
    "s = te.create_schedule(B.op)\n",
    "xo, xi = s[B].split(B.op.axis[0], factor=32)\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1950a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m], [stride_1], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, 32) {\n",
      "    for (i.inner: int32, 0, floordiv((m + 31), 32)) {\n",
      "      if @tir.likely(((i.inner + (i.outer*floordiv((m + 31), 32))) < m), dtype=bool) {\n",
      "        B[((i.inner + (i.outer*floordiv((m + 31), 32)))*stride_1)] = A[((i.inner + (i.outer*floordiv((m + 31), 32)))*stride)]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also split a axis by nparts, which splits the axis contrary with factor.\n",
    "A = te.placeholder((m,), name=\"A\")\n",
    "B = te.compute((m,), lambda i: A[i], name=\"B\")\n",
    "\n",
    "s = te.create_schedule(B.op)\n",
    "bx, tx = s[B].split(B.op.axis[0], nparts=32)\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97088e",
   "metadata": {},
   "source": [
    "## tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2339510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((m + 9), 10)) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "      for (i.inner: int32, 0, 10) {\n",
      "        if @tir.likely((((i.outer*10) + i.inner) < m), dtype=bool) {\n",
      "          for (j.inner: int32, 0, 5) {\n",
      "            if @tir.likely((((j.outer*5) + j.inner) < n), dtype=bool) {\n",
      "              let cse_var_2: int32 = ((j.outer*5) + j.inner)\n",
      "              let cse_var_1: int32 = ((i.outer*10) + i.inner)\n",
      "              B[((cse_var_1*stride_1) + (cse_var_2*stride_3))] = A[((cse_var_1*stride) + (cse_var_2*stride_2))]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tile help you execute the computation tile by tile over two axes.\n",
    "A = te.placeholder((m, n), name=\"A\")\n",
    "B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "\n",
    "s = te.create_schedule(B.op)\n",
    "xo, yo, xi, yi = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a3507",
   "metadata": {},
   "source": [
    "## fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c977c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((m + 9), 10)) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "      for (i.inner.j.inner.fused: int32, 0, 50) {\n",
      "        if @tir.likely((((i.outer*10) + floordiv(i.inner.j.inner.fused, 5)) < m), dtype=bool) {\n",
      "          if @tir.likely((((j.outer*5) + floormod(i.inner.j.inner.fused, 5)) < n), dtype=bool) {\n",
      "            let cse_var_2: int32 = ((j.outer*5) + floormod(i.inner.j.inner.fused, 5))\n",
      "            let cse_var_1: int32 = ((i.outer*10) + floordiv(i.inner.j.inner.fused, 5))\n",
      "            B[((cse_var_1*stride_1) + (cse_var_2*stride_3))] = A[((cse_var_1*stride) + (cse_var_2*stride_2))]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fuse can fuse two consecutive axes of one computation.\n",
    "A = te.placeholder((m, n), name=\"A\")\n",
    "B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "\n",
    "s = te.create_schedule(B.op)\n",
    "# tile to four axes first: (i.outer, j.outer, i.inner, j.inner)\n",
    "xo, yo, xi, yi = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)\n",
    "# then fuse (i.inner, j.inner) into one axis: (i.inner.j.inner.fused)\n",
    "fused = s[B].fuse(xi, yi)\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd453b1e",
   "metadata": {},
   "source": [
    "## reorder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "352fb2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.inner: int32, 0, 10) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "      for (i.outer: int32, 0, floordiv((m + 9), 10)) {\n",
      "        if @tir.likely((((i.outer*10) + i.inner) < m), dtype=bool) {\n",
      "          for (j.inner: int32, 0, 5) {\n",
      "            if @tir.likely((((j.outer*5) + j.inner) < n), dtype=bool) {\n",
      "              let cse_var_2: int32 = ((j.outer*5) + j.inner)\n",
      "              let cse_var_1: int32 = ((i.outer*10) + i.inner)\n",
      "              B[((cse_var_1*stride_1) + (cse_var_2*stride_3))] = A[((cse_var_1*stride) + (cse_var_2*stride_2))]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reorder can reorder the axes in the specified order.\n",
    "A = te.placeholder((m, n), name=\"A\")\n",
    "B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "\n",
    "s = te.create_schedule(B.op)\n",
    "# tile to four axes first: (i.outer, j.outer, i.inner, j.inner)\n",
    "xo, yo, xi, yi = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)\n",
    "# then reorder the axes: (i.inner, j.outer, i.outer, j.inner)\n",
    "s[B].reorder(xi, yo, xo, yi)\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d33834",
   "metadata": {},
   "source": [
    "## bind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dc71126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*n)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [n], [stride_1], type=\"auto\")} {\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = floordiv((n + 63), 64);\n",
      "  attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 64;\n",
      "  if @tir.likely((((blockIdx.x*64) + threadIdx.x) < n), dtype=bool) {\n",
      "    B[(((blockIdx.x*64) + threadIdx.x)*stride_1)] = (A[(((blockIdx.x*64) + threadIdx.x)*stride)]*2f32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bind can bind a specified axis with a thread axis, often used in gpu programming.\n",
    "A = te.placeholder((n,), name=\"A\")\n",
    "B = te.compute(A.shape, lambda i: A[i] * 2, name=\"B\")\n",
    "\n",
    "s = te.create_schedule(B.op)\n",
    "bx, tx = s[B].split(B.op.axis[0], factor=64)\n",
    "s[B].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
    "s[B].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4ba19",
   "metadata": {},
   "source": [
    "## compute_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26c17fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m], [stride_1], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m], [stride_2], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    B[(i*stride_1)] = (A[(i*stride)] + 1f32)\n",
      "  }\n",
      "  for (i_1: int32, 0, m) {\n",
      "    C[(i_1*stride_2)] = (B[(i_1*stride_1)]*2f32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For a schedule that consists of multiple operators, TVM will compute tensors at the root separately by default.\n",
    "A = te.placeholder((m,), name=\"A\")\n",
    "B = te.compute((m,), lambda i: A[i] + 1, name=\"B\")\n",
    "C = te.compute((m,), lambda i: B[i] * 2, name=\"C\")\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66009f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m], [stride_1], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m], [stride_2], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    B[(i*stride_1)] = (A[(i*stride)] + 1f32)\n",
      "    C[(i*stride_2)] = (B[(i*stride_1)]*2f32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute_at can move computation of B into the first axis of computation of C.\n",
    "A = te.placeholder((m,), name=\"A\")\n",
    "B = te.compute((m,), lambda i: A[i] + 1, name=\"B\")\n",
    "C = te.compute((m,), lambda i: B[i] * 2, name=\"C\")\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "s[B].compute_at(s[C], C.op.axis[0])\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66192d41",
   "metadata": {},
   "source": [
    "## compute_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc33614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m], [stride_1], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m], [stride_2], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    C[(i*stride_2)] = ((A[(i*stride)] + 1f32)*2f32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute_inline can mark one stage as inline, then the body of computation will be expanded and inserted at the address where the tensor is required.\n",
    "A = te.placeholder((m,), name=\"A\")\n",
    "B = te.compute((m,), lambda i: A[i] + 1, name=\"B\")\n",
    "C = te.compute((m,), lambda i: B[i] * 2, name=\"C\")\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "s[B].compute_inline()\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ad25c",
   "metadata": {},
   "source": [
    "## compute_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d58c8356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m], [stride_1], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m], [stride_2], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    B[(i*stride_1)] = (A[(i*stride)] + 1f32)\n",
      "  }\n",
      "  for (i_1: int32, 0, m) {\n",
      "    C[(i_1*stride_2)] = (B[(i_1*stride_1)]*2f32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute_root can move computation of one stage to the root.\n",
    "A = te.placeholder((m,), name=\"A\")\n",
    "B = te.compute((m,), lambda i: A[i] + 1, name=\"B\")\n",
    "C = te.compute((m,), lambda i: B[i] * 2, name=\"C\")\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "s[B].compute_at(s[C], C.op.axis[0])\n",
    "s[B].compute_root()\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33bdd5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "为了获得性能良好的内核实现，一般的工作流程往往是：\n",
    "* 通过一系列compute操作描述计算。 \n",
    "* 尝试使用原语来schedule。 \n",
    "* 编译运行看看性能差异。 \n",
    "* 根据运行结果调整schedule。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
